# Taller 1 â€“ GCP para IngenierÃ­a de Datos

> **Marca de agua**: **Ruben Dario Quispe Vilca**

---

## ğŸ“‡ Metadatos
- **Nombre**: Ruben Quispe  
- **Centro de Estudios**: DMC  
- **Curso**: GCP para ingenierÃ­a de datos - Brangovich OrdoÃ±ez  
- **VersiÃ³n**: 1.0  
- **Fecha**: 29/08/2025

---

## ğŸ§¾ Resumen
Este repositorio despliega de forma **automÃ¡tica** una instancia de **Cloud Data Fusion (Basic)**, crea un **bucket** en GCS, un **dataset** en **BigQuery** y **despliega/ejecuta** el pipeline *Caso2*.  
El despliegue se realiza con **Cloud Build** usando `cloudbuild.yaml` y scripts auxiliares.

---

## ğŸ—‚ï¸ Estructura del repositorio
```
.
â”œâ”€ cloudbuild.yaml               # Pipeline de Cloud Build (idempotente)
â”œâ”€ data/                         # Archivos de datos de ejemplo
â”‚  â”œâ”€ customers.txt
â”‚  â”œâ”€ orders.csv
â”‚  â”œâ”€ products.csv
â”‚  â””â”€ promotions.json
â”œâ”€ datafusion/
â”‚  â””â”€ pipeline.json             # Pipeline en formato AppRequest (CDAP)
â”œâ”€ scripts/
â”‚  â”œâ”€ wait_for_cdf.sh           # Espera a que CDF estÃ© RUNNING
â”‚  â”œâ”€ deploy_pipeline.sh        # Despliega el pipeline vÃ­a REST
â”‚  â””â”€ start_pipeline.sh         # Dispara una ejecuciÃ³n del pipeline
â”œâ”€ bq/
â”‚  â””â”€ curated_sales_schema.json # (Opcional) esquema explÃ­cito para la tabla
â”œâ”€ run_deploy.ps1               # *One-click* deploy (Windows/PowerShell)
â”œâ”€ cleanup.ps1                  # Limpieza total (Windows/PowerShell)
â”œâ”€ run_deploy.sh                # *One-click* deploy (Linux/macOS)
â””â”€ cleanup.sh                   # Limpieza total (Linux/macOS)
```

---

## âœ… Prerrequisitos
- **Proyecto GCP** con **facturaciÃ³n** habilitada.
- **gcloud CLI** autenticado: `gcloud auth login`
- Permisos para: *Cloud Build, Data Fusion, BigQuery, Storage, Service Usage.*
- (Opcional) **VS Code** como entorno de trabajo.

---

## ğŸš€ Despliegue (Windows / VS Code)
En una terminal PowerShell ubicada en la carpeta del repo:

```powershell
# Reemplaza el ProjectId por el tuyo si aplica
powershell -ExecutionPolicy Bypass -File .\run_deploy.ps1 -ProjectId taller-1-gcp-ruben
```

El script:
1. Establece el proyecto y habilita **Cloud Build**.
2. Concede roles necesarios a la SA de Cloud Build.
3. Ejecuta `gcloud builds submit` con sustituciones por defecto:
   - `_REGION=us-central1`
   - `_BQ_LOCATION=US`
   - `_CDF_INSTANCE=taller1-cdf`
   - `_BQ_DATASET=retail_curated`
   - `_BUCKET=$Env:USERNAME-taller1-raw`

> Los pasos son idempotentes: si el bucket/dataset/instancia existe, continÃºa sin fallar.

---

## â–¶ï¸ Volver a ejecutar el pipeline (sin redeploy)
```powershell
$REGION   = "us-central1"
$INSTANCE = "taller1-cdf"
$ENDPOINT = (gcloud beta data-fusion instances describe $INSTANCE --location $REGION --format "value(apiEndpoint)")
$TOKEN    = (gcloud auth print-access-token)

# Dispara la ejecuciÃ³n
Invoke-WebRequest -Method Post `
  -Uri "$ENDPOINT/v3/namespaces/default/apps/Caso2/workflows/DataPipelineWorkflow/start" `
  -Headers @{Authorization="Bearer $TOKEN"} `
  -Body "{{}}" -ContentType "application/json"
```

---

## ğŸ” VerificaciÃ³n en BigQuery
```powershell
bq ls retail_curated
bq query --use_legacy_sql=false "SELECT COUNT(*) AS rows FROM `retail_curated.curated_sales`"
bq query --use_legacy_sql=false "SELECT * FROM `retail_curated.curated_sales` LIMIT 10"
```

---

## ğŸ§¹ Limpiar recursos (empezar de cero)
> **Â¡Cuidado!** borra la instancia CDF, el bucket y el dataset.

```powershell
powershell -ExecutionPolicy Bypass -File .\cleanup.ps1 -ProjectId taller-1-gcp-ruben
```

---

## ğŸ§  Notas tÃ©cnicas
- Data Fusion se maneja con **`gcloud beta data-fusion`** y `--edition=basic`.
- El deploy a CDAP usa **AppRequest** (artifact `cdap-data-pipeline` con versiÃ³n de la instancia).
- `cloudbuild.yaml` aÃ±ade un paso `chmod` para evitar `Permission denied` en scripts.
- Rutas GCS de las fuentes se parametrizan con `$\{{_BUCKET}}` y se resuelven en build.

---

## ğŸ› ï¸ SoluciÃ³n de problemas comunes
- **409 Bucket exists**: es esperado; el paso continÃºa.
- **Instancia CDF se queda en CREATING**: verificar APIs habilitadas y billing.
- **HTTP 400 en deploy (schema)**: asegurar que `BigQuerySink.properties.schema` sea JSON Avro vÃ¡lido. Este repo ya lo incluye correcto.
- **Permisos**: confirmar que la SA de Cloud Build tenga *datafusion.admin, bigquery.admin, storage.admin, serviceusage.serviceUsageAdmin, dataproc.editor*.

---

## Â© Autor
**Ruben Quispe** â€” *DMC*  
*Marca de agua*: **Ruben Dario Quispe Vilca**

> Este README fue generado para el curso **â€œGCP para ingenierÃ­a de datos - Brangovich OrdoÃ±ezâ€**.  
> VersiÃ³n **1.0** â€“ 29/08/2025
